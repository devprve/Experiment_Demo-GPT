{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "app is a text classification model that uses a pre-trained transformer-based language model, specifically a DistilBERT model, to generate embeddings for text documents."
      ],
      "metadata": {
        "id": "AuFp2htLdkA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lenpZ9XRiamH",
        "outputId": "ffa04b82-4666-4184-ca76-8eeb5f7c561e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yHMHSgYzqCm",
        "outputId": "012cf089-8a48-4f0e-8566-5061bb929cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "8h7LQjvM5M6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "# Load a smaller subset of the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "documents, labels = newsgroups.data[:1000], newsgroups.target[:1000]\n",
        "\n",
        "# Load a smaller and more efficient transformer model\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate embeddings for text documents\n",
        "def generate_embeddings(texts, batch_size=32):\n",
        "    cache_dir = '.embeddings_cache'\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_path = os.path.join(cache_dir, hashlib.md5('\\n'.join(texts).encode('utf-8')).hexdigest() + '.npy')\n",
        "    if os.path.exists(cache_path):\n",
        "        embeddings = np.load(cache_path)\n",
        "    else:\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            input_ids = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids.to(device))\n",
        "            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
        "        embeddings = np.concatenate(embeddings)\n",
        "        np.save(cache_path, embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings for the text documents\n",
        "X = generate_embeddings(documents)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSFivwt35s9G",
        "outputId": "3a9218da-4336-4dff-ebfd-f5e3d033dada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.45      0.50        11\n",
            "           1       0.11      0.20      0.14        10\n",
            "           2       0.45      0.56      0.50         9\n",
            "           3       0.50      0.24      0.32        17\n",
            "           4       0.00      0.00      0.00         7\n",
            "           5       0.62      0.62      0.62        13\n",
            "           6       0.50      0.62      0.56         8\n",
            "           7       0.50      0.18      0.27        11\n",
            "           8       0.18      0.55      0.27        11\n",
            "           9       0.50      0.33      0.40        12\n",
            "          10       0.60      0.46      0.52        13\n",
            "          11       0.20      0.30      0.24        10\n",
            "          12       0.23      0.43      0.30         7\n",
            "          13       0.67      0.40      0.50        10\n",
            "          14       1.00      0.42      0.59        12\n",
            "          15       0.75      0.75      0.75         8\n",
            "          16       0.17      0.11      0.13         9\n",
            "          17       1.00      0.50      0.67         4\n",
            "          18       0.80      0.50      0.62         8\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.38       200\n",
            "   macro avg       0.47      0.38      0.39       200\n",
            "weighted avg       0.47      0.38      0.39       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5 0 0 0 0 0 0 0 2 0 1 1 0 0 0 0 2 0 0 0]\n",
            " [0 2 1 1 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 5 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 3 0 4 4 1 2 0 1 1 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 4 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 0 1 8 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 1 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 3 0 0 2 2 0 0 2 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 1 0 1 0 6 0 0 1 1 0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0 0 3 4 2 1 0 0 0 0 0 0 0 0]\n",
            " [2 1 0 1 0 0 0 0 3 0 6 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 1 1 0 0 1 1 0 3 2 0 0 0 0 0 0 0]\n",
            " [0 0 2 0 0 0 1 0 0 0 0 1 3 0 0 0 0 0 0 0]\n",
            " [0 3 0 0 1 0 0 0 1 0 0 1 0 4 0 0 0 0 0 0]\n",
            " [0 1 0 0 1 0 0 0 2 0 0 1 1 1 5 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 6 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 2 1 2 0 0 2 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 4 0]\n",
            " [0 1 0 0 0 0 0 0 5 0 0 0 0 0 0 2 1 0 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "# Load a smaller subset of the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "documents, labels = newsgroups.data[:1000], newsgroups.target[:1000]\n",
        "\n",
        "# Load a smaller and more efficient transformer model\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate embeddings for text documents\n",
        "def generate_embeddings(texts, batch_size=32):\n",
        "    cache_dir = '.embeddings_cache'\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_path = os.path.join(cache_dir, hashlib.md5('\\n'.join(texts).encode('utf-8')).hexdigest() + '.npy')\n",
        "    if os.path.exists(cache_path):\n",
        "        embeddings = np.load(cache_path)\n",
        "    else:\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            input_ids = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids.to(device))\n",
        "            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
        "        embeddings = np.concatenate(embeddings)\n",
        "        np.save(cache_path, embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings for the text documents\n",
        "X = generate_embeddings(documents)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFy6W2n4Bsju",
        "outputId": "f4b40ae2-a16c-452e-8a15-137369159787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.45      0.50        11\n",
            "           1       0.11      0.20      0.14        10\n",
            "           2       0.45      0.56      0.50         9\n",
            "           3       0.50      0.24      0.32        17\n",
            "           4       0.00      0.00      0.00         7\n",
            "           5       0.62      0.62      0.62        13\n",
            "           6       0.50      0.62      0.56         8\n",
            "           7       0.50      0.18      0.27        11\n",
            "           8       0.18      0.55      0.27        11\n",
            "           9       0.50      0.33      0.40        12\n",
            "          10       0.60      0.46      0.52        13\n",
            "          11       0.20      0.30      0.24        10\n",
            "          12       0.23      0.43      0.30         7\n",
            "          13       0.67      0.40      0.50        10\n",
            "          14       1.00      0.42      0.59        12\n",
            "          15       0.75      0.75      0.75         8\n",
            "          16       0.17      0.11      0.13         9\n",
            "          17       1.00      0.50      0.67         4\n",
            "          18       0.80      0.50      0.62         8\n",
            "          19       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.38       200\n",
            "   macro avg       0.47      0.38      0.39       200\n",
            "weighted avg       0.47      0.38      0.39       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5 0 0 0 0 0 0 0 2 0 1 1 0 0 0 0 2 0 0 0]\n",
            " [0 2 1 1 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 5 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 3 0 4 4 1 2 0 1 1 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 4 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 0 1 8 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 1 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 3 0 0 2 2 0 0 2 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 1 0 1 0 6 0 0 1 1 0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0 0 3 4 2 1 0 0 0 0 0 0 0 0]\n",
            " [2 1 0 1 0 0 0 0 3 0 6 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 1 1 0 0 1 1 0 3 2 0 0 0 0 0 0 0]\n",
            " [0 0 2 0 0 0 1 0 0 0 0 1 3 0 0 0 0 0 0 0]\n",
            " [0 3 0 0 1 0 0 0 1 0 0 1 0 4 0 0 0 0 0 0]\n",
            " [0 1 0 0 1 0 0 0 2 0 0 1 1 1 5 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 6 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 2 1 2 0 0 2 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 4 0]\n",
            " [0 1 0 0 0 0 0 0 5 0 0 0 0 0 0 2 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above app is a text classification model that uses a pre-trained transformer-based language model, specifically a DistilBERT model, to generate embeddings for text documents. These embeddings are then used as features to train a support vector machine (SVM) classifier to classify the text documents into categories."
      ],
      "metadata": {
        "id": "mK2fXed1BPk0"
      }
    }
  ]
}